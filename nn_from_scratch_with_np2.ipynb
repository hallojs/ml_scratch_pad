{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zw1jIj5O10SH"
   },
   "source": [
    "# Neuronal Network from Scratch with numpy Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_No9Csms22Wx"
   },
   "source": [
    "This notebook is only a minimal extension of the first notebook. The only thing we add here is a second output neuron. The activation function, the data and the rest of the architecture remain the same.\n",
    "\n",
    "## Architecture\n",
    "<img src=\"img/nn_2.png\" style=\"height:250px\">\n",
    "\n",
    "## Loss Function\n",
    "As loss-function we use again the Mean Squared Error $L=\\frac{1}{n}\\sum_{i=1}^n (y_1-\\hat{y}_1)^2 + (y_2-\\hat{y}_2)^2$, with $\\hat{y}_i=o_i$ beeing the predicted labels (genders) and $y_i$ beeing the given labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIa08I1y1qO1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 437,
     "status": "ok",
     "timestamp": 1578176190206,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -60
    },
    "id": "j8ZwIgMdOqwQ",
    "outputId": "9a4ac973-d52b-4dc3-db32-287a22c618ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:\n",
      "      name  weight  height gender\n",
      "0    Alice     133      65      F\n",
      "1      Bob     160      72      M\n",
      "2  Charlie     152      70      M\n",
      "3    Diana     120      60      F\n",
      "\n",
      "prediction_data:\n",
      "    name  weight  height\n",
      "0  Emily     128      63\n",
      "1  Frank     155      68\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.DataFrame({'name':   ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "                              'weight': [133, 160, 152, 120],\n",
    "                              'height': [65, 72, 70, 60],\n",
    "                              'gender': ['F', 'M', 'M', 'F']})\n",
    "print('training_data:')\n",
    "print(training_data)\n",
    "\n",
    "# replace 'F' with 1 and 'M' with 0\n",
    "training_data['gender'].replace(to_replace=['F','M'], value=[1,0], inplace=True)\n",
    "\n",
    "# normalize weight and height\n",
    "for i in ['weight', 'height']:\n",
    "  training_data[i] = (training_data[i] - \n",
    "                             training_data[i].mean())\\\n",
    "                           / training_data[i].std()\n",
    "\n",
    "data = training_data[['weight', 'height']].to_numpy()\n",
    "\n",
    "all_y_trues = training_data['gender'].to_numpy()\n",
    "\n",
    "# F - (1,0), M - (0,1)\n",
    "all_y_trues = list(zip(all_y_trues, 1-all_y_trues))\n",
    "\n",
    "prediction_data = pd.DataFrame({'name':   ['Emily', 'Frank'],\n",
    "                                'weight': [128, 155],\n",
    "                                'height': [63, 68]})\n",
    "print('\\nprediction_data:')\n",
    "print(prediction_data)\n",
    "\n",
    "# normalize weight and height\n",
    "for i in ['weight', 'height']:\n",
    "  prediction_data[i] = (prediction_data[i] - \n",
    "                             prediction_data[i].mean())\\\n",
    "                           / prediction_data[i].std()\n",
    "\n",
    "pred_data = prediction_data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1247516,
     "status": "ok",
     "timestamp": 1578177669929,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -60
    },
    "id": "8FqUkKRbPDE8",
    "outputId": "c59a2739-5f28-4253-a541-24b97bff9bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.58667\n",
      "Epoch 500 loss: 0.54219\n",
      "Epoch 1000 loss: 0.50183\n",
      "Epoch 1500 loss: 0.46734\n",
      "Epoch 2000 loss: 0.43918\n",
      "Epoch 2500 loss: 0.41667\n",
      "Epoch 3000 loss: 0.39858\n",
      "Epoch 3500 loss: 0.38368\n",
      "Epoch 4000 loss: 0.37095\n",
      "Epoch 4500 loss: 0.35963\n",
      "Epoch 5000 loss: 0.34926\n",
      "Epoch 5500 loss: 0.33953\n",
      "Epoch 6000 loss: 0.33027\n",
      "Epoch 6500 loss: 0.32137\n",
      "Epoch 7000 loss: 0.31280\n",
      "Epoch 7500 loss: 0.30452\n",
      "Epoch 8000 loss: 0.29653\n",
      "Epoch 8500 loss: 0.28882\n",
      "Epoch 9000 loss: 0.28141\n",
      "Epoch 9500 loss: 0.27428\n",
      "Epoch 10000 loss: 0.26745\n",
      "Epoch 10500 loss: 0.26091\n",
      "Epoch 11000 loss: 0.25465\n",
      "Epoch 11500 loss: 0.24868\n",
      "Epoch 12000 loss: 0.24299\n",
      "Epoch 12500 loss: 0.23757\n",
      "Epoch 13000 loss: 0.23241\n",
      "Epoch 13500 loss: 0.22752\n",
      "Epoch 14000 loss: 0.22287\n",
      "Epoch 14500 loss: 0.21847\n",
      "Epoch 15000 loss: 0.21429\n",
      "Epoch 15500 loss: 0.21034\n",
      "Epoch 16000 loss: 0.20661\n",
      "Epoch 16500 loss: 0.20307\n",
      "Epoch 17000 loss: 0.19974\n",
      "Epoch 17500 loss: 0.19658\n",
      "Epoch 18000 loss: 0.19361\n",
      "Epoch 18500 loss: 0.19080\n",
      "Epoch 19000 loss: 0.18816\n",
      "Epoch 19500 loss: 0.18567\n",
      "Epoch 20000 loss: 0.18332\n",
      "Epoch 20500 loss: 0.18111\n",
      "Epoch 21000 loss: 0.17903\n",
      "Epoch 21500 loss: 0.17708\n",
      "Epoch 22000 loss: 0.17524\n",
      "Epoch 22500 loss: 0.17351\n",
      "Epoch 23000 loss: 0.17189\n",
      "Epoch 23500 loss: 0.17037\n",
      "Epoch 24000 loss: 0.16894\n",
      "Epoch 24500 loss: 0.16760\n",
      "Epoch 25000 loss: 0.16635\n",
      "Epoch 25500 loss: 0.16518\n",
      "Epoch 26000 loss: 0.16408\n",
      "Epoch 26500 loss: 0.16305\n",
      "Epoch 27000 loss: 0.16209\n",
      "Epoch 27500 loss: 0.16119\n",
      "Epoch 28000 loss: 0.16036\n",
      "Epoch 28500 loss: 0.15958\n",
      "Epoch 29000 loss: 0.15886\n",
      "Epoch 29500 loss: 0.15818\n",
      "loss:  0.1575610955063579\n",
      "best_loss:  0.1575610955063579\n",
      "Emily [0.86183014 0.44250095]\n",
      "Frank [0.20836339 0.83390256]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_trues, y_preds):\n",
    "  # y_trues and y_preds are numpy arrays of the same length.\n",
    "  diff = np.square(y_trues - y_preds)\n",
    "  return np.mean(np.apply_along_axis(np.sum, 1, diff))\n",
    "\n",
    "class NeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 2 neurons (o1, o2)\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "    self.w7 = np.random.normal()\n",
    "    self.w8 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "    self.b4 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    o2 = sigmoid(self.w7 * h1 + self.w6 * h2 + self.b4)\n",
    "    return o1, o2\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a (n x 2) numpy array with n elements.\n",
    "    '''\n",
    "    learn_rate = 0.0005\n",
    "    epochs = 30000 # number of times to loop through the entire dataset\n",
    "    y_pred = [0,0]\n",
    "    \n",
    "    loss = 100\n",
    "\n",
    "    # plot loss during training\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    ax1.set_title('Mean Square Loss Error')\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred[0] = o1\n",
    "\n",
    "        sum_o2 = self.w7 * h1 + self.w8 * h2 + self.b4\n",
    "        o2 = sigmoid(sum_o2)\n",
    "        y_pred[1] = o2\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred1 = -2 * (y_true[0] - y_pred[0])\n",
    "        d_L_d_ypred2 = -2 * (y_true[1] - y_pred[1])\n",
    "\n",
    "        # - Neuron o1\n",
    "        d_ypred1_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred1_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred1_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # - Neuron o2\n",
    "        d_ypred2_d_w7 = h1 * deriv_sigmoid(sum_o2)\n",
    "        d_ypred2_d_w8 = h2 * deriv_sigmoid(sum_o2)\n",
    "        d_ypred2_d_b4 = deriv_sigmoid(sum_o2)\n",
    "\n",
    "        # - Neuron h1\n",
    "        d_ypred1_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred2_d_h1 = self.w7 * deriv_sigmoid(sum_o2)\n",
    "        \n",
    "        # w2\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_ypred1_d_w2 = d_ypred1_d_h1 * d_h1_d_w2\n",
    "        d_ypred2_d_w2 = d_ypred2_d_h1 * d_h1_d_w2\n",
    "        d_L_d_w2 = d_L_d_ypred1 * d_ypred1_d_w2 + d_L_d_ypred2 * d_ypred2_d_w2\n",
    "\n",
    "        # w1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_ypred1_d_w1 = d_ypred1_d_h1 * d_h1_d_w1\n",
    "        d_ypred2_d_w1 = d_ypred2_d_h1 * d_h1_d_w1\n",
    "        d_L_d_w1 = d_L_d_ypred1 * d_ypred1_d_w1 + d_L_d_ypred2 * d_ypred2_d_w1\n",
    "\n",
    "        # Neuron h2\n",
    "        d_ypred1_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred2_d_h2 = self.w8 * deriv_sigmoid(sum_o2)\n",
    "\n",
    "        # w4\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_ypred1_d_w4 = d_ypred1_d_h2 * d_h2_d_w4\n",
    "        d_ypred2_d_w4 = d_ypred2_d_h2 * d_h2_d_w4\n",
    "        # w4 influences the loss function via y_pred1 as well as via y_pred2\n",
    "        d_L_d_w4 = d_L_d_ypred1 * d_ypred1_d_w4 + d_L_d_ypred2 * d_ypred2_d_w4\n",
    "\n",
    "        # w3\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_ypred1_d_w3 = d_ypred1_d_h2 * d_h2_d_w3\n",
    "        d_ypred2_d_w3 = d_ypred2_d_h2 * d_h2_d_w3\n",
    "        d_L_d_w3 = d_L_d_ypred1 * d_ypred1_d_w3 + d_L_d_ypred2 * d_ypred2_d_w3\n",
    "\n",
    "        # b2\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "        d_ypred1_d_b2 = d_ypred1_d_h2 * d_h2_d_b2\n",
    "        d_ypred2_d_b2 = d_ypred2_d_h2 * d_h2_d_b2\n",
    "        d_L_d_b2 = d_L_d_ypred1 * d_ypred1_d_b2 + d_L_d_ypred2 * d_ypred2_d_b2\n",
    "\n",
    "        # b1\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "        d_ypred1_d_b1 = d_ypred1_d_h1 * d_h1_d_b1\n",
    "        d_ypred2_d_b1 = d_ypred2_d_h1 * d_h1_d_b1\n",
    "        d_L_d_b1 = d_L_d_ypred1 * d_ypred1_d_b1 + d_L_d_ypred2 * d_ypred2_d_b1\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Weights\n",
    "        \n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred1 * d_ypred1_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred1 * d_ypred1_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred1 * d_ypred1_d_b3\n",
    "\n",
    "        # Neuron o2\n",
    "        self.w7 -= learn_rate * d_L_d_ypred2 * d_ypred2_d_w7\n",
    "        self.w8 -= learn_rate * d_L_d_ypred2 * d_ypred2_d_w8\n",
    "        self.b4 -= learn_rate * d_L_d_ypred2 * d_ypred2_d_b4\n",
    "\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_b2\n",
    "\n",
    "      # --- Calculate total loss\n",
    "      # get predictions for all samples from data\n",
    "      # apply feedforward along x axis\n",
    "      y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "      if loss < mse_loss(all_y_trues, y_preds):\n",
    "        break\n",
    "      loss = mse_loss(all_y_trues, y_preds)\n",
    "      ax1.scatter(epoch, loss, marker='.', c='b')\n",
    "\n",
    "      if epoch % 500 == 0:\n",
    "        print(\"Epoch %d loss: %.5f\" % (epoch, loss))\n",
    "\n",
    "    #plt.show()\n",
    "    print('loss: ', loss)\n",
    "\n",
    "# Train our neural network!\n",
    "best_network = NeuralNetwork()\n",
    "best_network.train(data, all_y_trues)\n",
    "best_loss = mse_loss(all_y_trues, np.apply_along_axis(best_network.feedforward, 1, data))\n",
    "\n",
    "#for i in range(1,10):\n",
    "#  network = NeuralNetwork()\n",
    "#  network.train(data, all_y_trues)\n",
    "#  if mse_loss(all_y_trues, np.apply_along_axis(best_network.feedforward, 1, data)) < best_loss:\n",
    "#    best_loss = mse_loss(all_y_trues, np.apply_along_axis(best_network.feedforward, 1, data))\n",
    "#    best_network = network\n",
    "\n",
    "print('best_loss: ', best_loss)\n",
    "\n",
    "# Make some predictions\n",
    "y_preds = np.apply_along_axis(best_network.feedforward, 1, pred_data[:,1:3])\n",
    "for i in range(len(y_preds)):\n",
    "  print(pred_data[i,0], y_preds[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_from_scratch_with_np2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
